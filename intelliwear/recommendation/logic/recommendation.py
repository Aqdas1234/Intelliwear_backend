# -*- coding: utf-8 -*-
"""recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10h1SD5jk0wXN3fECUsMlL9LQ7KfwYfA_
"""
'''
!pip install gensim
!pip install numpy==1.24.36
!pip install faiss-cpu==1.7.4
!pip install joblib
'''

import numpy as np
import gensim
import pandas as pd
import joblib
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')
nltk.download('wordnet')
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from gensim.models import Word2Vec
import faiss
from sklearn.feature_extraction.text import TfidfVectorizer
from itertools import product

class CBModel:
    def __init__(self, directory):
        # Data members (variables)
        # self.directory = directory
        self.modelPath = directory + '/word2vec_model.model'
        self.tfidfPath = directory + '/tfidf_model.pkl'
        self.indexPath = directory + '/faiss_index.index'
        self.vectorsPath = directory + '/product_vectors.npy'
        self.mappingsPath = directory + '/product_mapping.pkl'
        self.dataPath = directory + '/products.csv'

        # Model components (initially set to None or appropriate initial values)
        self.model = None
        self.tfidf = None
        self.product_mapping = None
        self.product_vectors = None
        self.index = None
        self.train_data = None

        if self.isTrained():
          self.model = self.getWord2vecModel()
          self.index = self.getFaissIndex()
          self.tfidf = self.getIdfModel()
          self.product_vectors = self.getVectors()
          self.train_data = self.getCsvFile()
          self.product_mapping = self.getMappings()
        else :
          self.train()

    def getMappings(self):
      mapping = joblib.load(self.mappingsPath)
      return mapping

    def saveMappings(self,mappings):
      joblib.dump(mappings, self.mappingsPath)

    def updateMappings(self, products):
      for product_id in products["ProductID"].unique():
          if product_id not in self.product_mapping:
              self.product_mapping[product_id] = len(self.product_mapping)
      self.saveMappings(self.product_mapping)

    def getVectors(self):
      vectors = np.load(self.vectorsPath)
      return vectors

    def saveVectors(self, vectors):
      np.save(self.vectorsPath, vectors)

    def updateVectors(self,new_vectors):
      updated_vectors = np.vstack([self.product_vectors, new_vectors])
      self.product_vectors = updated_vectors
      self.saveVectors(updated_vectors)

    def getCsvFile(self):
      df = pd.read_csv(self.dataPath)
      return df

    def saveCsvFile(self,df):
      df.to_csv(self.dataPath, index=False)

    def updateData(self,df):
      data = self.getCsvFile()
      df1 = pd.concat([data, df] ,ignore_index=True)
      self.train_data = df1
      self.saveCsvFile(df1)

    def cleanCsvFile(self,df):
      df = df[df['Status'] != 'inactive']
      df = df.reset_index(drop=True)
      return df

    def saveIdfModel(self):
      joblib.dump(self.tfidf, self.tfidfPath)

    def getIdfModel(self):
      tfidf = joblib.load(self.tfidfPath)
      return tfidf

    def updateIdf(self,cleaned_products ):
      self.tfidf.fit(cleaned_products)  # Update the vocabulary

      new_embedding = self.tfidf.transform(cleaned_products)

      with open(self.tfidfPath, 'wb') as f:
          joblib.dump(self.tfidf, f)

    def getWord2vecModel(self):
      word2vec = Word2Vec.load(self.modelPath)
      return word2vec

    def saveWord2vecModel(self):
      self.model.save(self.modelPath)

    def updateWord2vecModel(self,new_tokens):
      self.model.build_vocab(new_tokens, update=True)
      self.model.train(new_tokens, total_examples=len(new_tokens), epochs=5,start_alpha=0.00001, end_alpha=0.000001)
      self.saveWord2vecModel()

    def getFaissIndex(self):
      index = faiss.read_index(self.indexPath)
      return index

    def saveFaissIndex(self):
      faiss.write_index(self.index, self.indexPath)

    def updateFaissIndex(self ,vectors):
      combined_embeddings = np.vstack(vectors)

      self.index.add(combined_embeddings)
      faiss.write_index(self.index, self.indexPath)

    def isTrained(self):
      import os
      return os.path.isfile(self.modelPath)

    def train(self):
      self.train_data = self.getCsvFile()
      self.train_data = self.cleanCsvFile(self.train_data)
      self.saveCsvFile(self.train_data)
      self.train_data['combined_text'] = self.train_data.apply(self.combine_attributes, axis=1)
      cleaned_products = [self.preprocess(prod) for prod in self.train_data['combined_text']]

      tokenized_products = [word_tokenize(prod) for prod in cleaned_products]

      sentences=tokenized_products

      self.model = Word2Vec(sentences, vector_size=100, window=7, min_count=1, sg=0)

      self.model.train(sentences, total_examples=len(sentences), epochs=15 ,start_alpha=0.0001, end_alpha=0.00001)
      self.saveWord2vecModel()

      self.tfidf = TfidfVectorizer()
      self.tfidf.fit_transform(cleaned_products)
      self.saveIdfModel()

      self.product_vectors = [self.get_weighted_vector(prod) for prod in cleaned_products]
      self.product_vectors = np.vstack(self.product_vectors)

      self.saveVectors(self.product_vectors)

      combined_embeddings = np.vstack(self.product_vectors)
      d = combined_embeddings.shape[1]

      self.index = faiss.IndexFlatL2(d)

      self.index.add(combined_embeddings)
      self.saveFaissIndex()
      self.product_mapping = {product: idx for idx, product in enumerate(self.train_data["ProductID"].unique())}
      self.saveMappings(self.product_mapping)


    def preprocess(self,text):
      lemmatizer = WordNetLemmatizer()
      stop_words = set(stopwords.words('english'))
      text = text.lower()
      text = re.sub(r'[^a-zA-Z\s]', '', text)
      words = text.split()
      words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
      return ' '.join(words)

    def combine_attributes(self,row):
      attributes = [
          row['ProductName'],
          row['ProductBrand'],
          str(row['Price (INR)']),
          row['Gender'],
          row['Description']
      ]
      combined_text = ' '.join(str(attr) for attr in attributes if attr)
      return ' '.join(set(combined_text.split()))

    def find_similar(self,id, k=5 ):

      product_index = self.product_mapping[id]
      query_vector = self.product_vectors[product_index].reshape(1, -1)
      indices = self.index.search(query_vector, k+1)[1]

      result = [ self.train_data['ProductID'][i] for i in indices[0] if self.train_data['Status'][i] != 'inactive'][1:]
      c = 0
      while len(result) < 5:
        c=c+1
        indices = self.index.search(query_vector, k+1+(5*c-len(result)))[1]
        result = [self.train_data['ProductID'][i] for i in indices[0] if self.train_data['Status'][i] != 'inactive'][1:]

      return result[:k]

    def get_weighted_vector(self,text):
      vocab = self.tfidf.get_feature_names_out()
      words = text.split()
      word_vectors = []
      weights = []
      for word in words:
          if word in self.model.wv and word in vocab:
              word_vectors.append(self.model.wv[word])
              weights.append(self.tfidf.idf_[self.tfidf.vocabulary_[word]])
      if word_vectors:
          return np.average(word_vectors, axis=0, weights=weights)
      else:
          return np.zeros(self.model.vector_size)

    def getAll(self,ids):
      result = list()

      for i in ids:
        res = self.find_similar(i)
        result.append(res)

      return result

    def addProducts(self, data):
        data['combined_text'] = data.apply(self.combine_attributes, axis=1)

        cleaned_products = [self.preprocess(prod) for prod in data['combined_text']]
        tokenized_products = [word_tokenize(prod) for prod in cleaned_products]

        self.updateIdf(cleaned_products)
        self.updateWord2vecModel(tokenized_products)
        vectors = [self.get_weighted_vector(prod) for prod in cleaned_products]
        self.updateVectors(vectors)
        self.updateFaissIndex(vectors)
        data = data.drop(columns=['combined_text'])
        self.updateData(data)
        self.updateMappings(data)

    def delete(self, id):
        df = self.getCsvFile()
        df.loc[self.product_mapping[id], "Status"] = "inactive"
        self.saveCsvFile(df)
        self.train_data.loc[self.product_mapping[id], "Status"] = "inactive"


#path = 'intelliwear/recommendation/data'
#m = CBModel(path)
#m.delete('795d0053-bdc7-4a2a-a118-fcc0337a1c32')
#print(m.product_mapping)